{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24abf42d",
   "metadata": {},
   "source": [
    "# Obtain SST PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b07482",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get equatorial pacfic SST\n",
    "def get_pacific_SST():\n",
    "    SST_anom_nonan = SST_anom.where(np.isnan(SST_anom)==False, drop=True)\n",
    "\n",
    "    # Load ocean basin mask\n",
    "    basin = xr.open_dataset('http://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NODC/.WOA09/.Masks/.basin/dods')\n",
    "    basin = basin.rename({'X': 'longitude', 'Y': 'latitude'})\n",
    "\n",
    "    # Select pacific basin, interpolate to ERA5 coords, and mask all values not in the pacific\n",
    "    basin_surf = basin.basin[0]\n",
    "    basin_surf_interp = basin_surf.interp_like(SST_anom_nonan, method='nearest')\n",
    "    SST_pacific = SST_anom_nonan.where(basin_surf_interp == 2)\n",
    "\n",
    "    # Select equatorial (10N - 10S) pacific SST only\n",
    "    SST_eq_pacific = SST_pacific.sel(latitude=slice(-10,10))\n",
    "    SST_eq_pacific_norm = SST_eq_pacific / SST_eq_pacific.weighted(weight).std()\n",
    "    \n",
    "    return SST_eq_pacific_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain PCs/EOFs of SST anomalies\n",
    "def get_PCs_EOFs():\n",
    "    solver = xMCA(get_pacific_SST().rename({'latitude':'lat','longitude':'lon'}))\n",
    "    solver.solve()\n",
    "\n",
    "    # Find SST EOFs and put them in xarray DataArray\n",
    "    eof_SST = solver.eofs(n=25)['left']\n",
    "    eof_SST = eof_SST.rename({'lon':'longitude','lat':'latitude'})\n",
    "    eof_SST['mode'] = np.arange(0,25)\n",
    "    # eof_SST[:,:,0] = -eof_SST[:,:,0]\n",
    "    eof_SST[:,:,1] = -eof_SST[:,:,1]\n",
    "\n",
    "    # Find SST PCs and put them in xarray DataArray\n",
    "    pc_SST = solver.pcs(n=25)['left']\n",
    "    pc_SST['mode'] = np.arange(0,25)\n",
    "    # pc_SST[:,0] = -pc_SST[:,0]\n",
    "    pc_SST[:,1] = -pc_SST[:,1]\n",
    "    \n",
    "    return pc_SST, eof_SST\n",
    "\n",
    "# Use 1-2-1 low pass filter on PCs\n",
    "def filtered(PCs):\n",
    "    pc_filtered = np.zeros((len(PCs)-2,2))\n",
    "    filter_coefs = np.array([1,2,1])\n",
    "    for i in range(len(PCs) - 2):\n",
    "        for j in range(2):\n",
    "            pc_filtered[i,j] = np.sum(PCs[i:i+3,j] * filter_coefs / 4)\n",
    "\n",
    "    PCs_filtered_xr = xr.DataArray(data=pc_filtered,coords={'time':PCs.time[0:-2],'mode':[0,1]})\n",
    "    \n",
    "    return PCs_filtered_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c894d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain the rotated EOFs/PCs by using a linear combination of the first two modes (Takahashi 2011)\n",
    "def rotate(eofs):\n",
    "    e_pattern = (eofs.sel(mode=0) - eofs.sel(mode=1)) / np.sqrt(2)\n",
    "    c_pattern = (eofs.sel(mode=0) + eofs.sel(mode=1)) / np.sqrt(2)\n",
    "    return e_pattern, c_pattern\n",
    "\n",
    "pc_SST, eof_SST = get_PCs_EOFs()\n",
    "pc_SST = filtered(pc_SST)\n",
    "\n",
    "epc, cpc = rotate(pc_SST)\n",
    "epat, cpat = rotate(eof_SST)\n",
    "\n",
    "eof_patterns = xr.concat([epat,cpat],dim='mode').assign_coords({'mode':[0,1]}).T # 0 = e-pattern, 1 = c-pattern\n",
    "pc_patterns = xr.concat([epc,cpc],dim='mode').assign_coords({'mode':[0,1]}).T # 0 = e-pattern, 1 = c-pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67b7f86",
   "metadata": {},
   "source": [
    "# Obtain Radiation PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28606542",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regress y onto the principal components\n",
    "def regOnPcs(pcs,y,n,mode,pval_corr):\n",
    "    # regress onto PCs to get spatial maps\n",
    "    if pcs.ndim == 2:\n",
    "        if n == 0:\n",
    "            X = pcs.sel(mode=mode).values.reshape(-1,1)\n",
    "            m = [mode]\n",
    "        else:\n",
    "            X = pcs.sel(mode=slice(0,n)).to_pandas()\n",
    "            m = np.arange(0,n+1)\n",
    "\n",
    "        y_stack = y.stack(allpoints=('latitude','longitude')).dropna(dim='allpoints')\n",
    "        \n",
    "        reg = LinearRegression(fit_intercept=False).fit(X,y_stack)\n",
    "        \n",
    "        # Obtain pcals and apply FDR correction\n",
    "        if pval_corr == 'y':\n",
    "            pvals = []\n",
    "            for i in range(0,len(y_stack.allpoints)):\n",
    "                pvals.append(sm.OLS(y_stack[:,i].values,X).fit().pvalues)\n",
    "            pvals_fdr = fdr_corr(pvals)\n",
    "\n",
    "            reg_ds = xr.Dataset(data_vars={'coefs':(['dim_0','dim_1'],reg.coef_),\n",
    "                                           'pvals':(['dim_0','dim_1'],pvals_fdr)},\n",
    "                                  coords={'dim_0':y_stack.allpoints.values,\n",
    "                                          'dim_1':m})\n",
    "        else:\n",
    "            reg_ds = xr.Dataset(data_vars={'coefs':(['dim_0','dim_1'],reg.coef_)},\n",
    "                                coords={'dim_0':y_stack.allpoints.values,\n",
    "                                        'dim_1':m})\n",
    "        \n",
    "        reg_rename = reg_ds.rename({'dim_0':'allpoints','dim_1':'mode'})\n",
    "        reg_xr = reg_rename.reindex_like(y_stack)\n",
    "        reg_unstack = reg_xr.unstack('allpoints')\n",
    "        \n",
    "        return reg_unstack\n",
    "    \n",
    "    # regress onto EOFs to get principal components\n",
    "    elif pcs.ndim == 3:\n",
    "        X_stack = pcs.stack(allpoints=('latitude','longitude')).transpose('allpoints',...).dropna(dim='allpoints')\n",
    "        y_stack = y.stack(allpoints=('latitude','longitude')).transpose('allpoints',...).dropna(dim='allpoints')\n",
    "        a_stack, b_stack = xr.align(X_stack,y_stack)\n",
    "        \n",
    "        if n == 0:\n",
    "            reg = LinearRegression(fit_intercept=False).fit(a_stack.sel(mode=mode).values.reshape(-1,1),b_stack)\n",
    "            m = [mode]\n",
    "        else:\n",
    "            reg = LinearRegression(fit_intercept=False).fit(a_stack.sel(mode=slice(0,n)),b_stack)\n",
    "            m = np.arange(0,n+1)     \n",
    "        \n",
    "        reg_ds = xr.Dataset(data_vars={'coefs':(['dim_0','dim_1'],reg.coef_)},\n",
    "                              coords={'dim_0':y_stack.time.values,\n",
    "                                      'dim_1':m})\n",
    "        reg_rename = reg_ds.rename({'dim_0':'time','dim_1':'mode'})\n",
    "        \n",
    "        return reg_rename\n",
    "    \n",
    "def fdr_corr(pvals):\n",
    "    p_fdr = []\n",
    "    for i in pvals.mode:\n",
    "        p_fdr.append(multitest.fdrcorrection(pvals.sel(mode=i).stack(allpoints=('latitude','longitude')))[0])\n",
    "    \n",
    "    p_fdr_xr = xr.DataArray(data=p_fdr,coords={'mode':np.arange(0,len(pvals.mode)),'allpoints':pvals.stack(allpoints=('latitude','longitude')).allpoints.values})\n",
    "    p_fdr_xr = p_fdr_xr.reindex_like(pvals.stack(allpoints=('latitude','longitude'))).unstack('allpoints')\n",
    "    \n",
    "    return p_fdr_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130dd63d",
   "metadata": {},
   "source": [
    "# Time-series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2806322",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get global-mean, reconstructed, and equatorial pacific radiation time-series\n",
    "def get_time_series(data,PCs):\n",
    "    data_gm = global_mean(data).dropna(dim='time')#.rolling(time=12,center=True).mean().dropna(dim='time')\n",
    "    data_recon = (PCs / PCs.std(dim='time')).sel(mode=slice(0,1)).sum(dim='mode')\n",
    "    data_recon_align, data_gm_align = xr.align(data_recon, data_gm)#.rolling(time=12,center=True).mean().dropna(dim='time'), data_gm)\n",
    "    \n",
    "    data_pacific = data.where(basin_surf_interp == 2)\n",
    "    data_trop,_ = xr.align(global_mean(data_pacific.sel(latitude=slice(-10,10))), data_gm)#.rolling(time=12,center=True).mean(), data_gm)\n",
    "    data_nino34,_ = xr.align(global_mean(data.sel(latitude=slice(-5,5),longitude=slice(190,240))), data_gm)#.rolling(time=12,center=True).mean()\n",
    "\n",
    "    return data_pacific, data_trop, data_nino34, data_recon\n",
    "\n",
    "## Calculate autocorrelation of a time-series\n",
    "def autocorrelation(x,lag):\n",
    "    y_lag = []\n",
    "    \n",
    "    for i in lag:\n",
    "        x_lag = x.shift(time=i).dropna(dim='time',how='all')\n",
    "        a_lag, b = xr.align(x_lag, x)\n",
    "\n",
    "        reg,_ = st.pearsonr(a_lag,b)\n",
    "\n",
    "        y_lag.append(reg)\n",
    "        \n",
    "    y_lag_xr = xr.DataArray(data=y_lag,coords={'lag':lag})\n",
    "    auto = y_lag_xr.interp(lag=np.arange(lag[0],lag[-1],0.01))\n",
    "    \n",
    "    # Find e-folding lag\n",
    "    for i in range(0,len(auto)):\n",
    "        if auto[i] <= np.exp(-1):\n",
    "            return auto[i].lag.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get power spectral density using Welch's method\n",
    "def get_signal(ts_data):\n",
    "    data_gm    = ts_data[0]\n",
    "    data_recon = ts_data[1]\n",
    "    data_trop  = ts_data[2]\n",
    "    \n",
    "    recon_sig = signal.welch(data_recon,nperseg=120,noverlap=60)\n",
    "    gm_sig = signal.welch(data_gm,nperseg=120,noverlap=60)\n",
    "    trop_sig = signal.welch(data_trop,nperseg=120,noverlap=60)\n",
    "    return recon_sig,gm_sig,trop_sig\n",
    "\n",
    "## Get magnitude squared coherence\n",
    "def get_coherence(data_gm,data_recon):\n",
    "    aligned_gm,aligned_recon = xr.align(data_gm,data_recon)\n",
    "    f, Cxy = signal.coherence(aligned_gm,aligned_recon,nperseg=120,noverlap=60)\n",
    "    return f, Cxy\n",
    "\n",
    "## Obtain a confidence interval, specify the dof using phi\n",
    "def conf_int(data,phi):\n",
    "    ci = st.chi2.ppf([0.025,0.975],df=2*phi)\n",
    "    ci = 2*phi / ci\n",
    "    \n",
    "    return ci[0]*data, ci[1]*data\n",
    "\n",
    "## Return a normalized PSD\n",
    "def norm(data):\n",
    "    return data / data.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e7312",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get a white & red noise spectrum from data's variance n times, then average the 95-th percentile values\n",
    "def noise(data,n):        \n",
    "    x, y = xr.align(data,data.shift(time=1).dropna(dim='time'))\n",
    "    auto = st.pearsonr(x,y)[0]\n",
    "    \n",
    "    # Monte Carlo generation of white/red noise\n",
    "    wh_psd = []\n",
    "    rd_psd = []\n",
    "    for i in range(0,n+1):\n",
    "        wh_ts = np.random.normal(loc=0,scale=data.std(),size=len(data.time))\n",
    "        \n",
    "        rd_ts = []\n",
    "        for j in range(0,len(data.time)):\n",
    "            if j == 0:\n",
    "                rd_ts.append(wh_ts[j])\n",
    "            else:\n",
    "                calc = auto*rd_ts[j-1] + wh_ts[j]*np.sqrt(1-auto**2)\n",
    "                rd_ts.append(calc)\n",
    "                \n",
    "        f,ps_wh = signal.welch(wh_ts,nperseg=120,noverlap=60)\n",
    "        f,ps_rd = signal.welch(rd_ts,nperseg=120,noverlap=60)\n",
    "    \n",
    "    wh_psd.append(ps_wh)\n",
    "    rd_psd.append(ps_rd)\n",
    "    \n",
    "    return np.percentile(a=wh_psd,q=97.5,axis=0), np.percentile(a=rd_psd,q=97.5,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a75521",
   "metadata": {},
   "source": [
    "# Lagged Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9693dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform lagged regression (w/ the possibility of normal regression for lag = 0) on two datasets x and y.\n",
    "def lagged_regress(x,y,lag): \n",
    "    x_lag = x.shift(time=lag)\n",
    "    # Stack y if it hs dimensions of (lat, lon, t)\n",
    "    if y.ndim == 3:\n",
    "        y_stack = y.stack(allpoints=('latitude','longitude')).dropna(dim='allpoints')\n",
    "    else:\n",
    "        y_stack = y\n",
    "\n",
    "    # Align to remove nans in x and y\n",
    "    x_lag_nonan = x_lag.dropna(dim='time')\n",
    "    y_nonan = y_stack.dropna(dim='time')\n",
    "    a_lag, b_stack = xr.align(x_lag_nonan, y_nonan)\n",
    "\n",
    "    # Perform linear regression\n",
    "    if x.ndim == 1:\n",
    "        reg = LinearRegression().fit(a_lag.values.reshape(-1,1),b_stack)\n",
    "    else:\n",
    "        reg = LinearRegression().fit(a_lag.values,b_stack)\n",
    "    \n",
    "    reg_da = xr.DataArray(data=reg.coef_)\n",
    "    \n",
    "    # Place betas in xarray, unstack (if betas are 2D), and return\n",
    "    if reg_da.ndim == 2:\n",
    "        reg_rename = reg_da.rename({'dim_0':'allpoints','dim_1':'mode'})#.assign_coords({'mode':y.mode})\n",
    "        reg_reindex = reg_rename.reindex_like(y_stack)\n",
    "        reg_unstack = reg_reindex.unstack('allpoints')\n",
    "        \n",
    "        return reg_unstack\n",
    "    elif reg_da.ndim == 1 and len(reg_da) > 1:\n",
    "        reg_rename = reg_da.rename({'dim_0':'mode'}).assign_coords({'mode':y.mode})\n",
    "        reg_reindex = reg_rename.reindex_like(y_stack)\n",
    "        \n",
    "        return reg_reindex\n",
    "    else:\n",
    "        return reg_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert data from time to lag coords via lagged regression\n",
    "def time2lag(x,y,lag):\n",
    "    y_lag = []\n",
    "\n",
    "    for i in lag:\n",
    "        y_lag.append(lagged_regress(x,y,i))\n",
    "\n",
    "    y_lag_xr = xr.concat(y_lag,dim='lag').assign_coords({'lag':lag})\n",
    "    \n",
    "    return y_lag_xr.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eefa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get r2 between full and reconstructed radiation maps vs lag\n",
    "def get_r2_lags(full_rad_data,EOFs):\n",
    "    r2 = np.zeros([len(lag_list),len(EOFs.mode)])\n",
    "    for i in lag_list:\n",
    "        for j in EOFs.mode:\n",
    "            dR_align, EOFs_align = xr.align(full_rad_data,EOFs)\n",
    "            r2[i+18,j] = (sm.OLS(dR_align.sel(lag=i,mode=j).stack(allpoints=('latitude','longitude')).values,\n",
    "                             EOFs_align.sel(lag=i,mode=j).stack(allpoints=('latitude','longitude')).values,missing='drop').fit().rsquared)\n",
    "    \n",
    "    return xr.DataArray(data=r2,coords={'lag':lag_list,'mode':EOFs.mode})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
